
@misc{trochim_types_2025,
	title = {Types of {Reliability}},
	url = {https://conjointly.com/kb/types-of-reliability/},
	abstract = {There are 4 general classes of reliability estimates, each with pros \& cons; Inter-Rater or Inter-Observer, Test-Retest, Parallel-Forms \& Internal Consistency.},
	language = {en},
	urldate = {2025-01-20},
	author = {Trochim, William M. K.},
	month = jan,
	year = {2025},
}

@article{idreos_nodb_nodate,
	title = {{NoDB}: {Here} are my {Data} {Files}. {Here} are my {Queries}. {Where} are my {Results}?},
	url = {https://www.cidrdb.org/cidr2011/Papers/CIDR11_Paper7.pdf},
	abstract = {Database management systems (DBMS) provide incredible ﬂexibility and performance when it comes to query processing, scalability and accuracy. To fully exploit DBMS features, however, the user must deﬁne a schema, load the data, tune the system for the expected workload, and answer several questions. Should the database use a column-store, a row-store or some hybrid format? What indices should be created? All these questions make for a formidable and time-consuming hurdle, often deterring new applications or imposing high cost to existing ones. A characteristic example is that of scientiﬁc databases with huge data sets. The prohibitive initialization cost and complexity still forces scientists to rely on “ancient” tools for their data management tasks, delaying scientiﬁc understanding and progress.},
	language = {en},
	author = {Idreos, Stratos and Alagiannis, Ioannis and Johnson, Ryan and Ailamaki, Anastasia},
	pages = {57--68},
}

@article{alagiannis_nodb_2015,
	title = {{NoDB}: efficient query execution on raw data files},
	volume = {58},
	issn = {0001-0782},
	shorttitle = {{NoDB}},
	url = {https://dl.acm.org/doi/10.1145/2830508},
	doi = {10.1145/2830508},
	abstract = {As data collections become larger and larger, users are faced with increasing bottlenecks in their data analysis. More data means more time to prepare and to load the data into the database before executing the desired queries. Many applications already avoid using database systems, for example, scientific data analysis and social networks, due to the complexity and the increased data-to-query time, that is, the time between getting the data and retrieving its first useful results. For many applications data collections keep growing fast, even on a daily basis, and this data deluge will only increase in the future, where it is expected to have much more data than what we can move or store, let alone analyze.We here present the design and roadmap of a new paradigm in database systems, called NoDB, which do not require data loading while still maintaining the whole feature set of a modern database system. In particular, we show how to make raw data files a first-class citizen, fully integrated with the query engine. Through our design and lessons learned by implementing the NoDB philosophy over a modern Database Management Systems (DBMS), we discuss the fundamental limitations as well as the strong opportunities that such a research path brings. We identify performance bottlenecks specific for in situ processing, namely the repeated parsing and tokenizing overhead and the expensive data type conversion. To address these problems, we introduce an adaptive indexing mechanism that maintains positional information to provide efficient access to raw data files, together with a flexible caching structure. We conclude that NoDB systems are feasible to design and implement over modern DBMS, bringing an unprecedented positive effect in usability and performance.\&lt;!-- END\_PAGE\_1 --\&gt;},
	number = {12},
	urldate = {2025-01-20},
	journal = {Commun. ACM},
	author = {Alagiannis, Ioannis and Borovica-Gajic, Renata and Branco, Miguel and Idreos, Stratos and Ailamaki, Anastasia},
	month = nov,
	year = {2015},
	pages = {112--121},
}

@inproceedings{noauthor_nodb_nodate,
	title = {{NoDB}: {Efficient} {Query} {Execution} on {Raw} {Data} {Files} {\textbar} {Proceedings} of the 2012 {ACM} {SIGMOD} {International} {Conference} on {Management} of {Data}},
	url = {https://dl.acm.org/doi/10.1145/2213836.2213864},
	doi = {https://dl.acm.org/doi/10.1145/2213836.2213864},
	language = {EN},
	urldate = {2024-06-04},
	booktitle = {{ACM} {Conferences}},
	doi = {10.1145/2213836.2213864},
	note = {Archive Location: world},
}

@misc{noauthor_tpc-homepage_2024,
	title = {{TPC}-{Homepage}},
	url = {https://www.tpc.org/default5.asp},
	urldate = {2024-11-19},
	month = nov,
	year = {2024},
}

@misc{postgresql_postgresql_2017,
	title = {{PostgreSQL}: {Creating} a {Database} {Cluster}},
	url = {https://www.postgresql.org/docs/9.2/creating-cluster.html},
	language = {en},
	urldate = {2024-10-19},
	journal = {PostgreSQL Documentation},
	author = {PostgreSQL},
	month = nov,
	year = {2017},
}

@misc{noauthor_postgresql_2021,
	title = {{PostgreSQL} {Release} 9.6.5},
	url = {https://www.postgresql.org/docs/9.6/release-9-6-5.html},
	language = {en},
	urldate = {2024-11-24},
	journal = {PostgreSQL Documentation},
	month = aug,
	year = {2021},
}

@misc{noauthor_hbpmedicalpostgresraw-ui_2018,
	title = {{HBPMedical}/{PostgresRAW}-{UI}},
	url = {https://github.com/HBPMedical/PostgresRAW-UI},
	urldate = {2024-12-29},
	publisher = {HBP Medical Informatics Platform},
	month = may,
	year = {2018},
	note = {Original-date: 2016-09-30T15:48:38Z},
}

@misc{noauthor_hbpmedicalpostgresraw-ui-docker_2016,
	title = {{HBPMedical}/{PostgresRAW}-{UI}-docker},
	url = {https://github.com/HBPMedical/PostgresRAW-UI-docker},
	urldate = {2024-12-29},
	publisher = {HBP Medical Informatics Platform},
	month = dec,
	year = {2016},
	note = {Original-date: 2016-12-06T10:27:01Z},
}

@misc{noauthor_hbpmedicalpostgresraw-docker_2018,
	title = {{HBPMedical}/{PostgresRAW}-docker},
	url = {https://github.com/HBPMedical/PostgresRAW-docker},
	abstract = {Docker container definition for PostgresRAW},
	urldate = {2024-12-29},
	publisher = {HBP Medical Informatics Platform},
	month = mar,
	year = {2018},
	note = {Original-date: 2016-11-25T13:56:11Z},
}

@misc{noauthor_hbpmedicalpostgresraw_2022,
	title = {{HBPMedical}/{PostgresRAW}},
	url = {https://github.com/HBPMedical/PostgresRAW},
	urldate = {2024-01-24},
	publisher = {HBP Medical Informatics Platform},
	month = jul,
	year = {2022},
	note = {Original-date: 2015-06-03T12:39:20Z},
}

@misc{huang_understand_2024,
	title = {Understand {PostgreSQL}’s {Planner} – {Simple} {Scan} {Paths} vs {Plans} - {Highgo} {Software} {Inc}.},
	url = {https://www.highgo.ca/2024/03/22/understand-postgresqls-planner-simple-scan-paths-vs-plans/},
	abstract = {Introduction When you send a query to PostgreSQL, it normally would go through stages of query processing and return you the results at the end. These stages are known as: Parse Analyze Rewrite Plan Execute I wrote another blog to briefly explain the responsibility of each query processing stage. You can find it here. In},
	language = {en-US},
	urldate = {2024-11-25},
	author = {Huang, Cary},
	month = mar,
	year = {2024},
}

@misc{huang_comprehensive_2024,
	title = {A {Comprehensive} {Overview} of {PostgreSQL} {Query} {Processing} {Stages} - {Highgo} {Software} {Inc}.},
	url = {https://www.highgo.ca/2024/01/26/a-comprehensive-overview-of-postgresql-query-processing-stages/},
	abstract = {Introduction When you send a query to PostgreSQL, it will undergo several processing stages in the backend. Each of these stages has different responsibilities to ensure that you receive correct responses in shortest amount of time possible. Yes, they can be quite large and complex to fully understand but I believe it is important for},
	language = {en-US},
	urldate = {2024-11-25},
	author = {Huang, Cary},
	month = jan,
	year = {2024},
}

@misc{noauthor_technology_2024,
	title = {Technology 2024 {Stack} {Overflow} {Developer} {Survey}},
	url = {https://survey.stackoverflow.co/2024/technology#most-popular-technologies-database},
	language = {en},
	urldate = {2024-11-23},
	journal = {Stack Overflow Developer Survey},
	month = nov,
	year = {2024},
}

@misc{noauthor_historical_2024,
	title = {Historical trend of the popularity ranking of database management systems},
	url = {https://db-engines.com/en/ranking_trend},
	urldate = {2024-11-23},
	journal = {DB-Engines Ranking},
	month = nov,
	year = {2024},
}

@misc{noauthor_data_2018,
	title = {Data {Age} 2025: the datasphere and data-readiness from edge to core},
	shorttitle = {Data {Age} 2025},
	url = {https://www.i-scoop.eu/big-data-action-value-context/data-age-2025-datasphere/},
	abstract = {A look at Data Age 2025, a collaboration between Seagate and IDC - on the growth of the global datasphere to 175 zettabytes by 2025.},
	language = {en-US},
	urldate = {2024-09-18},
	journal = {i-SCOOP},
	month = nov,
	year = {2018},
}

@misc{noauthor_postgresql_2024,
	title = {{PostgreSQL}: {Backend} {Flowchart}},
	url = {https://www.postgresql.org/developer/backend/},
	urldate = {2024-11-25},
	month = nov,
	year = {2024},
}

@misc{noauthor_database_nodate,
	title = {Database {Utilities} - {Oracle}  external table},
	url = {https://docs.oracle.com/cd/B19306_01/server.102/b14215/et_concepts.htm},
	language = {en},
	urldate = {2024-11-24},
}

@misc{noauthor_tpc-h_nodate,
	title = {{TPC}-{H} {Homepage}},
	url = {https://www.tpc.org/tpch/},
	urldate = {2024-12-28},
}

@misc{huang_overview_2023,
	title = {Overview of {PostgreSQL} {Foreign} {Data} {Wrapper} ({FDW}) - {Highgo} {Software} {Inc}.},
	url = {https://www.highgo.ca/2023/10/06/overview-of-postgresql-foreign-data-wrapper-fdw/, https://www.highgo.ca/2023/10/06/overview-of-postgresql-foreign-data-wrapper-fdw/},
	abstract = {Introduction A Foreign Data Wrapper (FDW) in PostgreSQL is an extension that allows you to access and manipulate data stored in external data sources as if they were tables within your PostgreSQL database. FDWs enable PostgreSQL to integrate with various data storage systems, both relational and non-relational, and present the data in a unified manner},
	language = {en-US},
	urldate = {2024-12-12},
	author = {Huang, Cary},
	month = oct,
	year = {2023},
}

@article{sheth_federated_1990,
	title = {Federated database systems for managing distributed, heterogeneous, and autonomous databases},
	volume = {22},
	issn = {0360-0300},
	url = {https://dl.acm.org/doi/10.1145/96602.96604},
	doi = {10.1145/96602.96604},
	abstract = {A federated database system (FDBS) is a collection of cooperating database systems that are autonomous and possibly heterogeneous. In this paper, we define a reference architecture for distributed database management systems from system and schema viewpoints and show how various FDBS architectures can be developed. We then define a methodology for developing one of the popular architectures of an FDBS. Finally, we discuss critical issues related to developing and operating an FDBS.},
	number = {3},
	urldate = {2024-12-11},
	journal = {ACM Comput. Surv.},
	author = {Sheth, Amit P. and Larson, James A.},
	month = sep,
	year = {1990},
	pages = {183--236},
}

@article{oracle_corporation_performant_2011,
	title = {performant and scalable data loading with {Oracle}},
	url = {https://www.oracle.com/technetwork/database/bi-datawarehousing/twpdwbestpractices-for-loading-11g-404400.pdf},
	language = {en},
	journal = {Oracle Corporation},
	author = {Oracle Corporation},
	year = {2011},
}

@misc{noauthor_overview_2021,
	title = {Overview},
	url = {https://www.postgresql.org/docs/9.6/catalogs-overview.html},
	language = {en},
	urldate = {2024-12-08},
	journal = {PostgreSQL Documentation},
	month = aug,
	year = {2021},
}

@article{mannino_statistical_1988,
	title = {Statistical profile estimation in database systems},
	volume = {20},
	issn = {0360-0300},
	url = {https://dl.acm.org/doi/10.1145/62061.62063},
	doi = {10.1145/62061.62063},
	abstract = {A statistical profile summarizes the instances of a database. It describes aspects such as the number of tuples, the number of values, the distribution of values, the correlation between value sets, and the distribution of tuples among secondary storage units. Estimation of database profiles is critical in the problems of query optimization, physical database design, and database performance prediction. This paper describes a model of a database of profile, relates this model to estimating the cost of database operations, and surveys methods of estimating profiles. The operators and objects in the model include build profile, estimate profile, and update profile. The estimate operator is classified by the relational algebra operator (select, project, join), the property to be estimated (cardinality, distribution of values, and other parameters), and the underlying method (parametric, nonparametric, and ad-hoc). The accuracy, overhead, and assumptions of methods are discussed in detail. Relevant research in both the database and the statistics disciplines is incorporated in the detailed discussion.},
	number = {3},
	urldate = {2024-12-08},
	journal = {ACM Comput. Surv.},
	author = {Mannino, Michael V. and Chu, Paicheng and Sager, Thomas},
	month = sep,
	year = {1988},
	pages = {191--221},
}

@misc{contributor_8_2024,
	title = {8 {Best} {Database} {Optimization} {Techniques}},
	url = {https://www.dnsstuff.com/database-optimization},
	abstract = {Gain valuable insights that will enable you to optimize your database performance, and keep up with users’ expectations of your business.},
	language = {en-US},
	urldate = {2024-12-08},
	journal = {Software Reviews, Opinions, and Tips - DNSstuff},
	author = {Contributor, Staff},
	month = nov,
	year = {2024},
}

@misc{noauthor_explain_2021,
	title = {{EXPLAIN}},
	url = {https://www.postgresql.org/docs/9.6/sql-explain.html},
	language = {en},
	urldate = {2024-12-07},
	journal = {PostgreSQL Documentation},
	month = aug,
	year = {2021},
}

@misc{noauthor_bruce_nodate,
	title = {Bruce {Momjian}: {Postgres} {Technical} {Performance} {Presentations}},
	url = {https://momjian.us/main/presentations/performance.html#optimizer},
	urldate = {2024-12-07},
}

@misc{noauthor_parallel_2021,
	title = {Parallel {Plans}},
	url = {https://www.postgresql.org/docs/9.6/parallel-plans.html},
	language = {en},
	urldate = {2024-12-06},
	journal = {PostgreSQL Documentation},
	month = aug,
	year = {2021},
}

@misc{noauthor_query_2021,
	title = {Query {Planning}},
	url = {https://www.postgresql.org/docs/9.6/runtime-config-query.html},
	language = {en},
	urldate = {2024-12-06},
	journal = {PostgreSQL Documentation},
	month = aug,
	year = {2021},
}

@article{momjian_explaining_nodate,
	title = {Explaining the {Postgres} {Query} {Optimizer}},
	language = {en},
	author = {Momjian, Bruce},
}

@misc{noauthor_819_2024,
	title = {8.19. {Object} {Identifier} {Types}},
	url = {https://www.postgresql.org/docs/17/datatype-oid.html},
	abstract = {8.19.\&nbsp;Object Identifier Types \# Object identifiers (OIDs) are used internally by PostgreSQL as primary keys for various system tables. Type …},
	language = {en},
	urldate = {2024-11-27},
	journal = {PostgreSQL Documentation},
	month = nov,
	year = {2024},
}

@misc{noauthor_backend_nodate,
	title = {Backend flowchart - {PostgreSQL} wiki},
	url = {https://wiki.postgresql.org/wiki/Backend_flowchart#main},
	urldate = {2024-11-25},
}

@misc{huang_trace_2019,
	title = {Trace {Query} {Processing} {Internals} with {Debugger} - {Highgo} {Software} {Inc}.},
	url = {https://www.highgo.ca/2019/10/03/trace-query-processing-internals-with-debugger/, https://www.highgo.ca/2019/10/03/trace-query-processing-internals-with-debugger/},
	abstract = {1. Overview In this article we will use GDB debugger to trace the internals of Postgres and observe how an input query passes through several levels of transformation (Parser -{\textgreater} Analyzer -{\textgreater} Rewriter -{\textgreater} Planner -{\textgreater} Executor) and eventually produces an output. This article is based on PG12 running on Ubuntu 18.04, and we will},
	language = {en-US},
	urldate = {2024-11-25},
	author = {Huang, Cary},
	month = oct,
	year = {2019},
}

@article{abedjan_profiling_2015,
	title = {Profiling relational data: a survey},
	volume = {24},
	issn = {0949-877X},
	shorttitle = {Profiling relational data},
	url = {https://doi.org/10.1007/s00778-015-0389-y},
	doi = {10.1007/s00778-015-0389-y},
	abstract = {Profiling data to determine metadata about a given dataset is an important and frequent activity of any IT professional and researcher and is necessary for various use-cases. It encompasses a vast array of methods to examine datasets and produce metadata. Among the simpler results are statistics, such as the number of null values and distinct values in a column, its data type, or the most frequent patterns of its data values. Metadata that are more difficult to compute involve multiple columns, namely correlations, unique column combinations, functional dependencies, and inclusion dependencies. Further techniques detect conditional properties of the dataset at hand. This survey provides a classification of data profiling tasks and comprehensively reviews the state of the art for each class. In addition, we review data profiling tools and systems from research and industry. We conclude with an outlook on the future of data profiling beyond traditional profiling tasks and beyond relational databases.},
	language = {en},
	number = {4},
	urldate = {2024-11-20},
	journal = {The VLDB Journal},
	author = {Abedjan, Ziawasch and Golab, Lukasz and Naumann, Felix},
	month = aug,
	year = {2015},
	keywords = {Association Rule, Data Profile, Frequent Itemsets, Minimal Uniques, Prefix Tree},
	pages = {557--581},
}

@misc{noauthor_141_2024,
	title = {14.1. {Using} {EXPLAIN}},
	url = {https://www.postgresql.org/docs/17/using-explain.html},
	abstract = {14.1.\&nbsp;Using EXPLAIN \# 14.1.1. EXPLAIN Basics 14.1.2. EXPLAIN ANALYZE 14.1.3. Caveats PostgreSQL devises a query plan for each query it …},
	language = {en},
	urldate = {2024-11-18},
	journal = {PostgreSQL Documentation},
	month = nov,
	year = {2024},
}

@misc{noauthor_postgresql_nodate,
	title = {{PostgreSQL} {Performance} {Tuning}: {Optimizing} {Database} {Indexes} {\textbar} {Timescale}},
	shorttitle = {{PostgreSQL} {Performance} {Tuning}},
	url = {https://www.timescale.com/learn/postgresql-performance-tuning-optimizing-database-indexes},
	abstract = {In the third part of our PostgreSQL Performance Tuning guide, we explore PostgreSQL indexes, including tips to optimize performance.},
	language = {en},
	urldate = {2024-11-15},
}

@misc{noauthor_513_2024,
	title = {5.13. {Foreign} {Data}},
	url = {https://www.postgresql.org/docs/17/ddl-foreign-data.html},
	abstract = {5.13.\&nbsp;Foreign Data \# PostgreSQL implements portions of the SQL/MED specification, allowing you to access data that resides outside PostgreSQL using …},
	language = {en},
	urldate = {2024-10-29},
	journal = {PostgreSQL Documentation},
	month = sep,
	year = {2024},
}

@article{rey_seamless_2023,
	title = {Seamless {Integration} of {Parquet} {Files} into {Data} {Processing}},
	url = {http://dl.gi.de/handle/20.500.12116/40316},
	doi = {10.18420/BTW2023-12},
	abstract = {Relational database systems are still the most powerful tool for data analysis. However, the steps necessary to bring existing data into the database make them unattractive for data exploration, especially when the data is stored in data lakes where users often use Parquet files, a binary column-oriented file format.},
	language = {en},
	urldate = {2024-10-27},
	author = {Rey, Alice and Freitag, Michael and Neumann, Thomas},
	year = {2023},
	note = {ISBN: 9783885797258
Publisher: Gesellschaft für Informatik e.V.},
}

@misc{noauthor_postgresql_nodate-1,
	title = {{PostgreSQL} {Source} {Code}: src/bin/initdb/initdb.c {File} {Reference}},
	url = {https://doxygen.postgresql.org/initdb_8c.html},
	urldate = {2024-10-21},
}

@article{noauthor_gdb_nodate,
	title = {{GDB} {Tutorial} - {A} {Walkthrough} with {Examples}},
	language = {en},
}

@misc{noauthor_postgresai_2024,
	title = {Postgres.{AI} / {PostgreSQL} {Consulting} / postgres-howtos · {GitLab}},
	url = {https://gitlab.com/postgres-ai/postgresql-consulting/postgres-howtos},
	abstract = {Postgres HowTo articles},
	language = {en},
	urldate = {2024-10-20},
	journal = {GitLab},
	month = aug,
	year = {2024},
}

@misc{noauthor_bruce_nodate-1,
	title = {Bruce {Momjian}: {Postgres} {Internals} {Presentations}},
	url = {https://momjian.us/main/presentations/internals.html},
	urldate = {2024-10-11},
}

@article{momjian_postgresql_nodate,
	title = {{PostgreSQL} {Internals} {Through} {Pictures}},
	abstract = {POSTGRESQL is an open-source, full-featured relational database. This presentation gives an overview of how POSTGRESQL processes queries.},
	language = {en},
	author = {Momjian, Bruce},
}

@misc{onojakpor_what_2024,
	title = {What {Is} {Metadata} in {Databases}: {All} {You} {Need} to {Know}},
	shorttitle = {What {Is} {Metadata} in {Databases}},
	url = {https://www.dbvis.com/thetable/what-is-metadata-in-databases-all-you-need-to-know/},
	abstract = {In this tutorial, we embark on a journey to unravel what metadata in databases is. Learn how to optimize and master your database metadata like never before!},
	language = {en-US},
	urldate = {2024-10-05},
	journal = {DbVisualizer},
	author = {Onojakpor, Ochuko},
	month = aug,
	year = {2024},
}

@inproceedings{cheng_parallel_2014,
	address = {Snowbird Utah USA},
	title = {Parallel in-situ data processing with speculative loading},
	isbn = {978-1-4503-2376-5},
	url = {https://dl.acm.org/doi/10.1145/2588555.2593673},
	doi = {10.1145/2588555.2593673},
	abstract = {Traditional databases incur a signiﬁcant data-to-query delay due to the requirement to load data inside the system before querying. Since this is not acceptable in many domains generating massive amounts of raw data, e.g., genomics, databases are entirely discarded. External tables, on the other hand, provide instant SQL querying over raw ﬁles. Their performance across a query workload is limited though by the speed of repeated full scans, tokenizing, and parsing of the entire ﬁle.},
	language = {en},
	urldate = {2024-10-05},
	booktitle = {Proceedings of the 2014 {ACM} {SIGMOD} {International} {Conference} on {Management} of {Data}},
	publisher = {ACM},
	author = {Cheng, Yu and Rusu, Florin},
	month = jun,
	year = {2014},
	pages = {1287--1298},
}

@article{noauthor_dinodb_nodate,
	series = {{Data4U} '14},
	title = {{DiNoDB} {\textbar} {Proceedings} of the {First} {International} {Workshop} on {Bringing} the {Value} of "{Big} {Data}" to {Users} ({Data4U} 2014)},
	shorttitle = {{DiNoDB}: {Efficient} {Large}-{Scale} {Raw} {Data} {Analytics}},
	url = {https://doi.org/10.1145/2658840.2658841},
	doi = {10.1145/2658840.2658841},
	abstract = {Modern big data workflows, found in e.g., machine learning use cases, often involve iterations of cycles of batch analytics and interactive analytics on temporary data. Whereas batch analytics solutions for large volumes of raw data are well established (e.g., Hadoop, MapReduce), state-of-the-art interactive analytics solutions (e.g., distributed shared nothing RDBMSs) require data loading and/or transformation phase, which is inherently expensive for temporary data.In this paper, we propose a novel scalable distributed solution for in-situ data analytics, that offers both scalable batch and interactive data analytics on raw data, hence avoiding the loading phase bottleneck of RDBMSs. Our system combines a MapReduce based platform with the recently proposed NoDB paradigm, which optimizes traditional centralized RDBMSs for in-situ queries of raw files. We revisit the NoDB's centralized design and scale it out supporting multiple clients and data processing nodes to produce a new distributed data analytics system we call Distributed NoDB (DiNoDB). DiNoDB leverages MapReduce batch queries to produce critical pieces of metadata (e.g., distributed positional maps and vertical indices) to speed up interactive queries without the overheads of the data loading and data movement phases allowing users to quickly and efficiently exploit their data.Our experimental analysis demonstrates that DiNoDB significantly reduces the data-to-query latency with respect to comparable state-of-the-art distributed query engines, like Shark, Hive and HadoopDB.},
	urldate = {2024-09-25},
	journal = {Proceedings of the First International Workshop on Bringing the Value of "Big Data" to Users (Data4U 2014)},
	pages = {1--6},
}

@article{alagiannis_nodb_nodate,
	title = {{NoDB}: {Efﬁcient} {Query} {Execution} on {Raw} {Data} {Files}},
	abstract = {As data collections become larger and larger, users are faced with increasing bottlenecks in their data analysis. More data means more time to prepare and to load the data into the database before executing the desired queries. Many applications already avoid using database systems, e.g., scientiﬁc data analysis and social networks, due to the complexity and the increased data-to-query time, i.e., the time between getting the data and retrieving its ﬁrst useful results. For many applications data collections keep growing fast, even on a daily basis, and this data deluge will only increase in the future, where it is expected to have much more data than what we can move or store, let alone analyze.},
	language = {en},
	author = {Alagiannis, Ioannis and Borovica-Gajic, Renata and Branco, Miguel and Idreos, Stratos and Ailamaki, Anastasia},
}

@misc{noauthor_chapter_2024,
	title = {Chapter 53. {System} {Catalogs}},
	url = {https://www.postgresql.org/docs/16/catalogs.html},
	abstract = {Chapter\&nbsp;53.\&nbsp;System Catalogs Table of Contents 53.1. Overview 53.2. pg\_aggregate 53.3. pg\_am 53.4. pg\_amop 53.5. pg\_amproc 53.6. pg\_attrdef 53.7. pg\_attribute 53.8. …},
	language = {en},
	urldate = {2024-09-25},
	journal = {PostgreSQL Documentation},
	month = aug,
	year = {2024},
}

@misc{southern_california_linux_expo_foreign_2020,
	title = {Foreign {Data} {Wrappers}: {Accessing} {External} {Data} from a {PostgreSQL} database},
	shorttitle = {Foreign {Data} {Wrappers}},
	url = {https://www.youtube.com/watch?v=ZY54dzKyfL4},
	abstract = {Talk by Gabrielle Roth

https://www.socallinuxexpo.org/scale/...

If you're managing multiple datastores and want to Just Use Postgres, foreign data wrappers are a good option. There are loads of them available, for nearly every datastore you can think of, and you can write your own if you can't find what you're looking for.This mostly-demo talk showcases file\_fdw, which allows you to query local resources from your database, and postgres\_fdw, which allows you to query other postgres databases. I'll cover use (and mis-use) cases, configuration of each, and interesting quirks.},
	urldate = {2024-09-25},
	author = {{Southern California Linux Expo}},
	month = mar,
	year = {2020},
}

@article{kersten_researchers_nodate,
	title = {The {Researcher}’s {Guide} to the {Data} {Deluge}: {Querying} a {Scientiﬁc} {Database} in {Just} a {Few} {Seconds}},
	abstract = {There is a clear need for interactive exploration of extremely large databases, especially in the area of scientiﬁc data management where ingestion of multiple Terabytes on a daily basis is foreseen. Unfortunately, current data management technology is not well-suited for such overwhelming demands.},
	language = {en},
	author = {Kersten, Martin L and Idreos, Stratos and Manegold, Stefan and Liarou, Erietta},
}

@article{lampropoulos_adaptive_2023,
	title = {Adaptive {Indexing} in {High}-{Dimensional} {Metric} {Spaces}},
	volume = {16},
	issn = {2150-8097},
	url = {https://dl.acm.org/doi/10.14778/3603581.3603592},
	doi = {10.14778/3603581.3603592},
	abstract = {Similarity search in high-dimensional metric spaces is routinely used in many applications including content-based image retrieval, bioinformatics, data mining, and recommender systems. Search can be accelerated by the use of an index. However, constructing a high-dimensional index can be quite expensive and may not pay off if the number of queries against the data is not large. In these circumstances, it is beneficial to construct an index adaptively, while responding to a query workload. Existing work on multidimensional adaptive indexing creates rectilinear space units by hyperplane-based partitioning. This approach, however, is highly ineffective in high-dimensional spaces. In this paper, we propose AV-tree: an alternative method for adaptive high-dimensional indexing that exploits previously computed distances, using query centers as vantage points. Our experimental study shows that AVtree yields cumulative cost for the first several hundred or even thousand queries much lower than that of pre-built indices. After thousands of queries, the per-query performance of the AV-tree converges or even surpasses that of the state-of-the-art MVP-tree. Arguably, our approach is commendable in environments where the expected number of queries is not large while there is a need to start answering queries as soon as possible, such as applications where data are updated frequently and past data soon become obsolete.},
	language = {en},
	number = {10},
	urldate = {2024-07-18},
	journal = {Proceedings of the VLDB Endowment},
	author = {Lampropoulos, Konstantinos and Zardbani, Fatemeh and Mamoulis, Nikos and Karras, Panagiotis},
	month = jun,
	year = {2023},
	pages = {2525--2537},
}

@article{mozaffari_self-tuning_2024,
	title = {Self-tuning {Database} {Systems}: {A} {Systematic} {Literature} {Review} of {Automatic} {Database} {Schema} {Design} and {Tuning}},
	volume = {56},
	issn = {0360-0300, 1557-7341},
	shorttitle = {Self-tuning {Database} {Systems}},
	url = {https://dl.acm.org/doi/10.1145/3665323},
	doi = {10.1145/3665323},
	abstract = {Self-tuning is a feature of autonomic databases that includes the problem of automatic schema design. It aims at providing an optimized schema that increases the overall database performance. While in relational databases automatic schema design focuses on the automated design of the physical schema, in NoSQL databases all levels of representation are considered: conceptual, logical, and physical. This is mainly because the latter are mostly schema-less and lack a standard schema design procedure as is the case for SQL databases. In this work, we carry out a systematic literature survey on automatic schema design in both SQL
              and
              NoSQL databases. We identify the levels of representation and the methods that are used for the schema design problem, and we present a novel taxonomy to classify and compare different schema design solutions. Our comprehensive analysis demonstrates that, despite substantial progress that has been made, schema design is still a developing field and considerable challenges need to be addressed, notably for NoSQL databases. We highlight the most important findings from the results of our analysis and identify areas for future research work.},
	language = {en},
	number = {11},
	urldate = {2024-07-18},
	journal = {ACM Computing Surveys},
	author = {Mozaffari, Maryam and Dignös, Anton and Gamper, Johann and Störl, Uta},
	month = nov,
	year = {2024},
	pages = {1--37},
}

@misc{smallcombe_postgresql_nodate,
	title = {{PostgreSQL} vs {MySQL}: {The} {Critical} {Differences}},
	shorttitle = {{PostgreSQL} vs {MySQL}},
	url = {https://www.integrate.io/blog/postgresql-vs-mysql-which-one-is-better-for-your-use-case/},
	abstract = {A comparison of MySQL and PostgreSQL to determine which is the better DBMS for your use case.},
	language = {en},
	urldate = {2024-07-04},
	journal = {Integrate.io},
	author = {Smallcombe, Mark},
}

@incollection{nambiar_benchmarking_2011,
	address = {Berlin, Heidelberg},
	title = {Benchmarking {Adaptive} {Indexing}},
	volume = {6417},
	isbn = {978-3-642-18205-1 978-3-642-18206-8},
	url = {http://link.springer.com/10.1007/978-3-642-18206-8_13},
	abstract = {Ideally, realizing the best physical design for the current and all subsequent workloads would impact neither performance nor storage usage. In reality, workloads and datasets can change dramatically over time and index creation impacts the performance of concurrent user and system activity. We propose a framework that evaluates the key premise of adaptive indexing — a new indexing paradigm where index creation and re-organization take place automatically and incrementally, as a side-eﬀect of query execution. We focus on how the incremental costs and beneﬁts of dynamic reorganization are distributed across the workload’s lifetime. We believe measuring the costs and utility of the stages of adaptation are relevant metrics for evaluating new query processing paradigms and comparing them to traditional approaches.},
	language = {en},
	urldate = {2024-07-03},
	booktitle = {Performance {Evaluation}, {Measurement} and {Characterization} of {Complex} {Systems}},
	publisher = {Springer Berlin Heidelberg},
	author = {Graefe, Goetz and Idreos, Stratos and Kuno, Harumi and Manegold, Stefan},
	editor = {Nambiar, Raghunath and Poess, Meikel},
	year = {2011},
	doi = {10.1007/978-3-642-18206-8_13},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {169--184},
}

@inproceedings{liarou_dbtouch_2014,
	address = {Chicago, IL, USA},
	title = {{dbTouch} in action database kernels for touch-based data exploration},
	isbn = {978-1-4799-2555-1},
	url = {http://ieeexplore.ieee.org/document/6816756/},
	doi = {10.1109/ICDE.2014.6816756},
	language = {en},
	urldate = {2024-06-30},
	booktitle = {2014 {IEEE} 30th {International} {Conference} on {Data} {Engineering}},
	publisher = {IEEE},
	author = {Liarou, Erietta and Idreos, Stratos},
	month = mar,
	year = {2014},
	pages = {1262--1265},
}

@article{idreos_dbtouch_nodate,
	title = {{dbTouch}: {Analytics} at your {Fingertips}},
	abstract = {As we enter the era of data deluge, turning data into knowledge has become the major challenge across most sciences and businesses that deal with data. In addition, as we increase our ability to create data, more and more people are confronted with data management problems on a daily basis for numerous aspects of every day life. A fundamental need is data exploration through interactive tools, i.e., being able to quickly and eﬀortlessly determine data and patterns of interest. However, modern database systems have not been designed with data exploration and usability in mind; they require users with expert knowledge and skills, while they react in a strict and monolithic way to every user request, resulting in correct answers but slow response times.},
	language = {en},
	author = {Idreos, Stratos and Liarou, Erietta},
}

@article{halim_indexing_nodate,
	title = {Indexing in {Main}-{Memory} {Column}-{Stores}},
	abstract = {Modern business applications and scientiﬁc databases call for inherently dynamic data storage environments. Such environments are characterized by two challenging features: (a) they have little idle system time to devote on physical design; and (b) there is little, if any, a priori workload knowledge, while the query and data workload keeps changing dynamically. In such environments, traditional approaches to index building and maintenance cannot apply. Database cracking has been proposed as a solution that allows on-the-ﬂy physical data reorganization, as a collateral effect of query processing. Cracking aims to continuously and automatically adapt indexes to the workload at hand, without human intervention. Indexes are built incrementally, adaptively, and on demand. Nevertheless, as we show, existing adaptive indexing methods fail to deliver workload-robustness; they perform much better with random workloads than with others. This frailty derives from the inelasticity with which these approaches interpret each query as a hint on how data should be stored. Current cracking schemes blindly reorganize the data within each query’s range, even if that results into successive expensive operations with minimal indexing beneﬁt.},
	language = {en},
	author = {Halim, Felix and Idreos, Stratos and Karras, Panagiotis and Yap, Roland H C},
}

@article{halim_indexing_nodate-1,
	title = {Indexing in {Main}-{Memory} {Column}-{Stores}},
	abstract = {Modern business applications and scientiﬁc databases call for inherently dynamic data storage environments. Such environments are characterized by two challenging features: (a) they have little idle system time to devote on physical design; and (b) there is little, if any, a priori workload knowledge, while the query and data workload keeps changing dynamically. In such environments, traditional approaches to index building and maintenance cannot apply. Database cracking has been proposed as a solution that allows on-the-ﬂy physical data reorganization, as a collateral effect of query processing. Cracking aims to continuously and automatically adapt indexes to the workload at hand, without human intervention. Indexes are built incrementally, adaptively, and on demand. Nevertheless, as we show, existing adaptive indexing methods fail to deliver workload-robustness; they perform much better with random workloads than with others. This frailty derives from the inelasticity with which these approaches interpret each query as a hint on how data should be stored. Current cracking schemes blindly reorganize the data within each query’s range, even if that results into successive expensive operations with minimal indexing beneﬁt.},
	language = {en},
	author = {Halim, Felix and Idreos, Stratos and Karras, Panagiotis and Yap, Roland H C},
}

@misc{google_techtalks_database_2007,
	title = {Database {Cracking}},
	url = {https://www.youtube.com/watch?v=Pdv4IgEuVrA},
	abstract = {Google Tech Talks
January 11, 2007

ABSTRACT

Database indices provide a non-discriminative navigational infrastructure to localize tuples of interest. Their maintenance cost is taken during database updates. In this work, we study the complementary approach, addressing index maintenance as part of query processing using continuous physical reorganization, i.e., cracking the database into manageable pieces. The motivation is that by automatically organizing data they way users request it, we can achieve fast access and the much desired self-organized behavior. This talk is based on the paper presented at CIDR2007.  Credits: Speaker:Stratos Idreos},
	urldate = {2024-06-16},
	author = {{Google TechTalks}},
	month = oct,
	year = {2007},
}

@misc{microsoft_research_database_2016,
	title = {Database {Cracking}},
	url = {https://www.youtube.com/watch?v=FQORtJtqovY},
	abstract = {Adaptive Indexing targets dynamic environments where there is no workload knowledge and there is not enough time to invest in physical design preparations and tuning, e.g., due to very large data sets. With adaptive indexing, each query is seen as an advice of how data should be stored. With each incoming query, data is reorganized on-the-fly as part of the query operators. Future queries can exploit and enhance this knowledge. Autonomously, adaptively and without any external human administration, the system continuously adjusts to ever changing workload patterns, updates and storage restrictions.   Adaptive indexing is designed on top of modern column-store architectures exploiting several new features such as one column at a time processing, vectorization,  late tuple reconstruction and cache conscious algorithms.},
	urldate = {2024-06-16},
	author = {{Microsoft Research}},
	month = aug,
	year = {2016},
}

@misc{halim_stochastic_2012,
	title = {Stochastic {Database} {Cracking}: {Towards} {Robust} {Adaptive} {Indexing} in {Main}-{Memory} {Column}-{Stores}},
	shorttitle = {Stochastic {Database} {Cracking}},
	url = {http://arxiv.org/abs/1203.0055},
	abstract = {Modern business applications and scientiﬁc databases call for inherently dynamic data storage environments. Such environments are characterized by two challenging features: (a) they have little idle system time to devote on physical design; and (b) there is little, if any, a priori workload knowledge, while the query and data workload keeps changing dynamically. In such environments, traditional approaches to index building and maintenance cannot apply. Database cracking has been proposed as a solution that allows on-the-ﬂy physical data reorganization, as a collateral effect of query processing. Cracking aims to continuously and automatically adapt indexes to the workload at hand, without human intervention. Indexes are built incrementally, adaptively, and on demand. Nevertheless, as we show, existing adaptive indexing methods fail to deliver workload-robustness; they perform much better with random workloads than with others. This frailty derives from the inelasticity with which these approaches interpret each query as a hint on how data should be stored. Current cracking schemes blindly reorganize the data within each query’s range, even if that results into successive expensive operations with minimal indexing beneﬁt.},
	language = {en},
	urldate = {2024-06-16},
	publisher = {arXiv},
	author = {Halim, Felix and Idreos, Stratos and Karras, Panagiotis and Yap, Roland H. C.},
	month = feb,
	year = {2012},
	note = {arXiv:1203.0055 [cs]},
	keywords = {Computer Science - Databases},
}

@inproceedings{pirk_database_2014,
	address = {Snowbird Utah USA},
	title = {Database cracking: fancy scan, not poor man's sort!},
	isbn = {978-1-4503-2971-2},
	shorttitle = {Database cracking},
	url = {https://dl.acm.org/doi/10.1145/2619228.2619232},
	doi = {10.1145/2619228.2619232},
	language = {en},
	urldate = {2024-06-16},
	booktitle = {Proceedings of the {Tenth} {International} {Workshop} on {Data} {Management} on {New} {Hardware}},
	publisher = {ACM},
	author = {Pirk, Holger and Petraki, Eleni and Idreos, Stratos and Manegold, Stefan and Kersten, Martin},
	month = jun,
	year = {2014},
	pages = {1--8},
}

@misc{idreos_database_2007,
	title = {Database {Cracking}},
	author = {Idreos, Stratos and Kersten, Martin and Manegold, Stefan},
	year = {2007},
	note = {Pages: 68-78
Place: Asilomar, California},
}

@inproceedings{schnaitter_colt_2006,
	address = {Chicago IL USA},
	title = {{COLT}: continuous on-line tuning},
	isbn = {978-1-59593-434-5},
	shorttitle = {{COLT}},
	url = {https://dl.acm.org/doi/10.1145/1142473.1142592},
	doi = {10.1145/1142473.1142592},
	language = {en},
	urldate = {2024-06-16},
	booktitle = {Proceedings of the 2006 {ACM} {SIGMOD} international conference on {Management} of data},
	publisher = {ACM},
	author = {Schnaitter, Karl and Abiteboul, Serge and Milo, Tova and Polyzotis, Neoklis},
	month = jun,
	year = {2006},
	pages = {793--795},
}

@article{olma_adaptive_2020,
	title = {Adaptive partitioning and indexing for in situ query processing},
	volume = {29},
	issn = {0949-877X},
	url = {https://doi.org/10.1007/s00778-019-00580-x},
	doi = {10.1007/s00778-019-00580-x},
	abstract = {The constant flux of data and queries alike has been pushing the boundaries of data analysis systems. The increasing size of raw data files has made data loading an expensive operation that delays the data-to-insight time. To alleviate the loading cost, in situ query processing systems operate directly over raw data and offer instant access to data. At the same time, analytical workloads have increasing number of queries. Typically, each query focuses on a constantly shifting—yet small—range. As a result, minimizing the workload latency requires the benefits of indexing in in situ query processing. In this paper, we present an online partitioning and indexing scheme, along with a partitioning and indexing tuner tailored for in situ querying engines. The proposed system design improves query execution time by taking into account user query patterns, to (i) partition raw data files logically and (ii) build lightweight partition-specific indexes for each partition. We build an in situ query engine called Slalom to showcase the impact of our design. Slalom employs adaptive partitioning and builds non-obtrusive indexes in different partitions on-the-fly based on lightweight query access pattern monitoring. As a result of its lightweight nature, Slalom achieves efficient query processing over raw data with minimal memory consumption. Our experimentation with both microbenchmarks and real-life workloads shows that Slalom outperforms state-of-the-art in situ engines and achieves comparable query response times with fully indexed DBMS, offering lower cumulative query execution times for query workloads with increasing size and unpredictable access patterns.},
	language = {en},
	number = {1},
	urldate = {2024-06-04},
	journal = {The VLDB Journal},
	author = {Olma, Matthaios and Karpathiotakis, Manos and Alagiannis, Ioannis and Athanassoulis, Manos and Ailamaki, Anastasia},
	month = jan,
	year = {2020},
	keywords = {Adaptive indexing, Logical partitioning, Online tuning},
	pages = {569--591},
}

@article{olma_slalom_2017,
	title = {Slalom: coasting through raw data via adaptive partitioning and indexing},
	volume = {10},
	issn = {2150-8097},
	shorttitle = {Slalom},
	url = {https://doi.org/10.14778/3115404.3115415},
	doi = {10.14778/3115404.3115415},
	abstract = {The constant flux of data and queries alike has been pushing the boundaries of data analysis systems. The increasing size of raw data files has made data loading an expensive operation that delays the data-to-insight time. Hence, recent in-situ query processing systems operate directly over raw data, alleviating the loading cost. At the same time, analytical workloads have increasing number of queries. Typically, each query focuses on a constantly shifting -- yet small -- range. Minimizing the workload latency, now, requires the benefits of indexing in in-situ query processing. In this paper, we present Slalom, an in-situ query engine that accommodates workload shifts by monitoring user access patterns. Slalom makes on-the-fly partitioning and indexing decisions, based on information collected by lightweight monitoring. Slalom has two key components: (i) an online partitioning and indexing scheme, and (ii) a partitioning and indexing tuner tailored for in-situ query engines. When compared to the state of the art, Slalom offers performance benefits by taking into account user query patterns to (a) logically partition raw data files and (b) build for each partition lightweight partition-specific indexes. Due to its lightweight and adaptive nature, Slalom achieves efficient accesses to raw data with minimal memory consumption. Our experimentation with both micro-benchmarks and real-life workloads shows that Slalom outperforms state-of-the-art in-situ engines (3 -- 10×), and achieves comparable query response times with fully indexed DBMS, offering much lower (∼ 3×) cumulative query execution times for query workloads with increasing size and unpredictable access patterns.},
	number = {10},
	urldate = {2024-01-25},
	journal = {Proceedings of the VLDB Endowment},
	author = {Olma, Matthaios and Karpathiotakis, Manos and Alagiannis, Ioannis and Athanassoulis, Manos and Ailamaki, Anastasia},
	month = jun,
	year = {2017},
	pages = {1106--1117},
}

@incollection{benczur_rawvis_2018,
	address = {Cham},
	title = {{RawVis}: {Visual} {Exploration} over {Raw} {Data}},
	volume = {11019},
	isbn = {978-3-319-98397-4 978-3-319-98398-1},
	shorttitle = {{RawVis}},
	url = {http://link.springer.com/10.1007/978-3-319-98398-1_4},
	abstract = {Data exploration and visual analytics systems are of great importance in Open Science scenarios, where less tech-savvy researchers wish to access and visually explore big raw data ﬁles (e.g., json, csv) generated by scientiﬁc experiments using commodity hardware and without being overwhelmed in the tedious processes of data loading, indexing and query optimization. In this work, we present our work for enabling efﬁcient in site query processing on big raw data ﬁles for interactive visual exploration scenarios. We introduce a framework, named RawVis, built on top of a lightweight in-memory tile-based index, VALINOR, that is constructed on-the-ﬂy given the ﬁrst user query over a raw ﬁle and adapted incrementally based on the user interaction. We evaluate the performance of prototype implementation compared to three other alternatives and show that our method outperforms in terms of response time, disk accesses and memory consumption.},
	language = {en},
	urldate = {2024-01-25},
	booktitle = {Advances in {Databases} and {Information} {Systems}},
	publisher = {Springer International Publishing},
	author = {Bikakis, Nikos and Maroulis, Stavros and Papastefanatos, George and Vassiliadis, Panos},
	editor = {Benczúr, András and Thalheim, Bernhard and Horváth, Tomáš},
	year = {2018},
	doi = {10.1007/978-3-319-98398-1_4},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {50--65},
}

@article{y_study_2014,
	title = {Study on {Potential} {Capabilities} of a {Nodb} {System}},
	volume = {3},
	issn = {23208465, 23197609},
	url = {http://www.airccse.org/journal/ijit/papers/3314ijit09.pdf},
	doi = {10.5121/ijit.2014.3309},
	abstract = {There is a need of optimal data to query processing technique to handle the increasing database size, complexity, diversity of use. With the introduction of commercial website, social network, expectations are that the high scalability, more flexible database will replace the RDBMS. Complex application and Big Table require highly optimized queries. Users are facing the increasing bottlenecks in their data analysis. A growing part of the database community recognizes the need for significant and fundamental changes to database design. A new philosophy for creating database systems called noDB aims at minimizing the datato-query time, most prominently by removing the need to load data before launching queries. That will process queries without any data preparation or loading steps. There may not need to store data. User can pipe raw data from websites, DBs, excel sheets into two promise sample inputs without storing anything. This study is based on PostgreSQL systems. A series of the baseline experiment are executed to evaluate the Performance of this system as per -a. Data loading cost, b-Query processing timing, c-Avoidance of Collision and Deadlock, d-Enabling the Big data storage and e-Optimize query processing etc. The study found significant possible capabilities of noDB system over the traditional database management system.},
	language = {en},
	number = {3},
	urldate = {2024-01-25},
	journal = {International Journal on Information Theory},
	author = {Y, Jayanta Singh and L, Kananbala Devi},
	month = jul,
	year = {2014},
	pages = {83--93},
}

@inproceedings{ge_speculative_2019,
	address = {New York, NY, USA},
	series = {{SIGMOD} '19},
	title = {Speculative {Distributed} {CSV} {Data} {Parsing} for {Big} {Data} {Analytics}},
	isbn = {978-1-4503-5643-5},
	url = {https://doi.org/10.1145/3299869.3319898},
	doi = {10.1145/3299869.3319898},
	abstract = {There has been a recent flurry of interest in providing query capability on raw data in today's big data systems. These raw data must be parsed before processing or use in analytics. Thus, a fundamental challenge in distributed big data systems is that of efficient parallel parsing of raw data. The difficulties come from the inherent ambiguity while independently parsing chunks of raw data without knowing the context of these chunks. Specifically, it can be difficult to find the beginnings and ends of fields and records in these chunks of raw data. To parallelize parsing, this paper proposes a speculation-based approach for the CSV format, arguably the most commonly used raw data format. Due to the syntactic and statistical properties of the format, speculative parsing rarely fails and therefore parsing is efficiently parallelized in a distributed setting. Our speculative approach is also robust, meaning that it can reliably detect syntax errors in CSV data. We experimentally evaluate the speculative, distributed parsing approach in Apache Spark using more than 11,000 real-world datasets, and show that our parser produces significant performance benefits over existing methods.},
	urldate = {2024-01-25},
	booktitle = {Proceedings of the 2019 {International} {Conference} on {Management} of {Data}},
	publisher = {Association for Computing Machinery},
	author = {Ge, Chang and Li, Yinan and Eilebrecht, Eric and Chandramouli, Badrish and Kossmann, Donald},
	month = jun,
	year = {2019},
	keywords = {csv, distributed, parallel, parsing},
	pages = {883--899},
}

@article{alagiannis_nodb_2012,
	title = {{NoDB} in action: adaptive query processing on raw data},
	volume = {5},
	issn = {2150-8097},
	shorttitle = {{NoDB} in action},
	url = {https://dl.acm.org/doi/10.14778/2367502.2367543},
	doi = {10.14778/2367502.2367543},
	abstract = {As data collections become larger and larger, users are faced with increasing bottlenecks in their data analysis. More data means more time to prepare the data, to load the data into the database and to execute the desired queries. Many applications already avoid using traditional database systems, e.g., scientific data analysis and social networks, due to their complexity and the increased data-to-query time, i.e. the time between getting the data and retrieving its first useful results. For many applications data collections keep growing fast, even on a daily basis, and this data deluge will only increase in the future, where it is expected to have much more data than what we can move or store, let alone analyze. In this demonstration, we will showcase a new philosophy for designing database systems called NoDB. NoDB aims at minimizing the data-to-query time, most prominently by removing the need to load data before launching queries. We will present our prototype implementation, PostgresRaw, built on top of PostgreSQL, which allows for efficient query execution over raw data files with zero initialization overhead. We will visually demonstrate how PostgresRaw incrementally and adaptively touches, parses, caches and indexes raw data files autonomously and exclusively as a side-effect of user queries.},
	number = {12},
	urldate = {2024-01-24},
	journal = {Proceedings of the VLDB Endowment},
	author = {Alagiannis, Ioannis and Borovica, Renata and Branco, Miguel and Idreos, Stratos and Ailamaki, Anastasia},
	month = aug,
	year = {2012},
	pages = {1942--1945},
}
